{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image as mp_image\n",
    "import seaborn as sns\n",
    "\n",
    "# Required magic to display matplotlib plots in notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#    for filename in filenames:\n",
    "#        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image, ImageOps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "working_dir = r\"C:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\shapes\"\n",
    "os.chdir(working_dir)\n",
    "img_size = (128, 128) #size of image fed into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to resize image\n",
    "def resize_image(src_image, size=(128,128), bg_color=\"white\"): \n",
    "    \n",
    "    # resize the image so the longest dimension matches our target size\n",
    "    src_image.thumbnail(size, PIL.Image.Resampling.LANCZOS)\n",
    "    \n",
    "    # Create a new square background image\n",
    "    new_image = Image.new(\"RGB\", size, bg_color)\n",
    "    \n",
    "    # Paste the resized image into the center of the square background\n",
    "    new_image.paste(src_image, (int((size[0] - src_image.size[0]) / 2), int((size[1] - src_image.size[1]) / 2)))\n",
    "  \n",
    "    # return the resized image\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming images...\n",
      "processing folder square\n",
      "processing folder triangle\n"
     ]
    }
   ],
   "source": [
    "training_folder_name = r\"C:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\shapes\"\n",
    "\n",
    "# New location for the resized images\n",
    "train_folder = '../working/data/natural_images'\n",
    "\n",
    "\n",
    "# Create the output folder if it doesn't already exist\n",
    "if os.path.exists(train_folder):\n",
    "    shutil.rmtree(train_folder)\n",
    "\n",
    "# Loop through each subfolder in the input folder\n",
    "print('Transforming images...')\n",
    "for root, folders, files in os.walk(training_folder_name):\n",
    "    for sub_folder in folders:\n",
    "        print('processing folder ' + sub_folder)\n",
    "        # Create a matching subfolder in the output dir\n",
    "        saveFolder = os.path.join(train_folder,sub_folder)\n",
    "        if not os.path.exists(saveFolder):\n",
    "            os.makedirs(saveFolder)\n",
    "        # Loop through the files in the subfolder\n",
    "        file_names = os.listdir(os.path.join(root,sub_folder))\n",
    "        for file_name in file_names:\n",
    "            # Open the file\n",
    "            file_path = os.path.join(root,sub_folder, file_name)\n",
    "            #print(\"reading \" + file_path)\n",
    "            image = Image.open(file_path)\n",
    "            # Create a resized version and save it\n",
    "            resized_image = resize_image(image, img_size)\n",
    "            saveAs = os.path.join(saveFolder, file_name)\n",
    "            #print(\"writing \" + saveAs)\n",
    "            resized_image.save(saveAs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_path):\n",
    "    # Load all the images\n",
    "    transformation = transforms.Compose([\n",
    "        # Randomly augment the image data\n",
    "            # Random horizontal flip\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "            # Random vertical flip\n",
    "        transforms.RandomVerticalFlip(0.3),\n",
    "        # transform to tensors\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize the pixel values (in R, G, and B channels)\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # Load all of the images, transforming them\n",
    "    full_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=transformation\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Split into training (70% and testing (30%) datasets)\n",
    "    train_size = int(0.7 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    \n",
    "    # use torch.utils.data.random_split for training/test split\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "    \n",
    "    # define a loader for the training data we can iterate through in 50-image batches\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=50,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # define a loader for the testing data we can iterate through in 50-image batches\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=50,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "        \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders ready to read ../working/data/natural_images\n"
     ]
    }
   ],
   "source": [
    "# Recall that we have resized the images and saved them into\n",
    "train_folder = '../working/data/natural_images'\n",
    "\n",
    "# Get the iterative dataloaders for test and training data\n",
    "train_loader, test_loader = load_dataset(train_folder)\n",
    "batch_size = train_loader.batch_size\n",
    "print(\"Data loaders ready to read\", train_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['square', 'triangle']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = sorted(os.listdir(training_folder_name))\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural net class\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    \n",
    "    # Defining the Constructor\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # In the init function, we define each layer we will use in our model\n",
    "        \n",
    "        # Our images are RGB, so we have input channels = 3. \n",
    "        # We will apply 12 filters in the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # A second convolutional layer takes 12 input channels, and generates 24 outputs\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # We in the end apply max pooling with a kernel size of 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # A drop layer deletes 20% of the features to help prevent overfitting\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.\n",
    "        # This means that our feature tensors are now 32 x 32, and we've generated 24 of them\n",
    "        \n",
    "        # We need to flatten these in order to feed them to a fully-connected layer\n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # In the forward function, pass the data through the layers we defined in the init function\n",
    "        \n",
    "        # Use a ReLU activation function after layer 1 (convolution 1 and pool)\n",
    "        x = F.relu(self.pool(self.conv1(x))) \n",
    "        \n",
    "        # Use a ReLU activation function after layer 2\n",
    "        x = F.relu(self.pool(self.conv2(x)))  \n",
    "        \n",
    "        # Select some features to drop to prevent overfitting (only drop during training)\n",
    "        x = F.dropout(self.drop(x), training=self.training)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 32 * 32 * 24)\n",
    "        # Feed to fully-connected layer to predict class\n",
    "        x = self.fc(x)\n",
    "        # Return class probabilities via a log_softmax function \n",
    "        return torch.log_softmax(x, dim=1)\n",
    "    \n",
    "device = \"cpu\"\n",
    "if (torch.cuda.is_available()):\n",
    "    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n",
    "    device = \"cuda\"\n",
    "\n",
    "# Create an instance of the model class and allocate it to the device\n",
    "model = Net(num_classes=len(classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_criteria = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128, 128])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_data[0][0].shape ########################################----------torch.Size([3, 128, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    print(\"Epoch:\", epoch)\n",
    "    # Process the images in batches\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Use the CPU or GPU as appropriate\n",
    "        # Recall that GPU is optimized for the operations we are dealing with\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Reset the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Push the data forward through the model layers\n",
    "        # print(data.shape)\n",
    "        output = model(data)\n",
    "        \n",
    "        # Get the loss\n",
    "        loss = loss_criteria(output, target)\n",
    "\n",
    "        # Keep a running total\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print metrics so we see some progress\n",
    "        # print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
    "    print('train loss:{:.6f}'.format(train_loss))        \n",
    "    # return average loss for the epoch\n",
    "    avg_loss = train_loss / (batch_idx+1)\n",
    "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        for data, target in test_loader:\n",
    "            batch_count += 1\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Get the predicted classes for this batch\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate the loss for this batch\n",
    "            test_loss += loss_criteria(output, target).item()\n",
    "            \n",
    "            # Calculate the accuracy for this batch\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += torch.sum(target==predicted).item()\n",
    "\n",
    "    # Calculate the average loss and total accuracy for this epoch\n",
    "    avg_loss = test_loss / batch_count\n",
    "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    # return average loss for the epoch\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n",
      "Epoch: 1\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([39, 3, 128, 128])\n",
      "train loss:11.914850\n",
      "Training set: Average loss: 0.113475\n",
      "Validation set: Average loss: 0.000000, Accuracy: 2246/2246 (100%)\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n",
      "Epoch: 1\n",
      "train loss:19.704153\n",
      "Training set: Average loss: 0.187659\n",
      "Validation set: Average loss: 0.001161, Accuracy: 2245/2246 (100%)\n",
      "\n",
      "Epoch: 2\n",
      "train loss:1.791833\n",
      "Training set: Average loss: 0.017065\n",
      "Validation set: Average loss: 0.002591, Accuracy: 2245/2246 (100%)\n",
      "\n",
      "Epoch: 3\n",
      "train loss:1.206416\n",
      "Training set: Average loss: 0.011490\n",
      "Validation set: Average loss: 0.000711, Accuracy: 2245/2246 (100%)\n",
      "\n",
      "Epoch: 4\n",
      "train loss:0.436264\n",
      "Training set: Average loss: 0.004155\n",
      "Validation set: Average loss: 0.000104, Accuracy: 2246/2246 (100%)\n",
      "\n",
      "Epoch: 5\n",
      "train loss:0.553444\n",
      "Training set: Average loss: 0.005271\n",
      "Validation set: Average loss: 0.000006, Accuracy: 2246/2246 (100%)\n",
      "\n",
      "Epoch: 6\n",
      "train loss:0.982023\n",
      "Training set: Average loss: 0.009353\n",
      "Validation set: Average loss: 0.000001, Accuracy: 2246/2246 (100%)\n",
      "\n",
      "Epoch: 7\n",
      "train loss:0.497322\n",
      "Training set: Average loss: 0.004736\n",
      "Validation set: Average loss: 0.000001, Accuracy: 2246/2246 (100%)\n",
      "\n",
      "Epoch: 8\n",
      "train loss:0.187783\n",
      "Training set: Average loss: 0.001788\n",
      "Validation set: Average loss: 0.000001, Accuracy: 2246/2246 (100%)\n",
      "\n",
      "Epoch: 9\n",
      "train loss:0.645864\n",
      "Training set: Average loss: 0.006151\n",
      "Validation set: Average loss: 0.000005, Accuracy: 2246/2246 (100%)\n",
      "\n",
      "Epoch: 10\n",
      "train loss:4.356007\n",
      "Training set: Average loss: 0.041486\n",
      "Validation set: Average loss: 0.000023, Accuracy: 2246/2246 (100%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use an \"Adam\" optimizer to adjust weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Specify the loss criteria\n",
    "# loss_criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "# Track metrics in these arrays\n",
    "epoch_nums = []\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "# Train over 10 epochs (We restrict to 10 for time issues)\n",
    "epochs = 10\n",
    "print('Training on', device)\n",
    "for epoch in range(1, epochs + 1):\n",
    "        train_loss = train(model, device, train_loader, optimizer, epoch)\n",
    "        test_loss = test(model, device, test_loader)\n",
    "        epoch_nums.append(epoch)\n",
    "        training_loss.append(train_loss)\n",
    "        validation_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\model_square_tri.h5\"\n",
    "torch.save(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\model_square_tri_script.h5\"\n",
    "torch.save(model.state_dict(), path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
