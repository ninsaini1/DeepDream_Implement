{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image as mp_image\n",
    "import seaborn as sns\n",
    "\n",
    "# Required magic to display matplotlib plots in notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#    for filename in filenames:\n",
    "#        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image, ImageOps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neeraj.saini\\AppData\\Local\\anaconda3\\envs\\py38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "working_dir = r\"C:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\"\n",
    "os.chdir(working_dir)\n",
    "img_size = (128, 128) #size of image fed into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to resize image\n",
    "def resize_image(src_image, size=(128,128), bg_color=\"white\"): \n",
    "    \n",
    "    # resize the image so the longest dimension matches our target size\n",
    "    src_image.thumbnail(size, PIL.Image.Resampling.LANCZOS)\n",
    "    \n",
    "    # Create a new square background image\n",
    "    new_image = Image.new(\"RGB\", size, bg_color)\n",
    "    \n",
    "    # Paste the resized image into the center of the square background\n",
    "    new_image.paste(src_image, (int((size[0] - src_image.size[0]) / 2), int((size[1] - src_image.size[1]) / 2)))\n",
    "  \n",
    "    # return the resized image\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming images...\n",
      "processing folder square\n",
      "processing folder triangle\n"
     ]
    }
   ],
   "source": [
    "training_folder_name = r\"C:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\shapes\"\n",
    "\n",
    "# New location for the resized images\n",
    "train_folder = '../working/data/natural_images'\n",
    "\n",
    "\n",
    "# Create the output folder if it doesn't already exist\n",
    "if os.path.exists(train_folder):\n",
    "    shutil.rmtree(train_folder)\n",
    "\n",
    "# Loop through each subfolder in the input folder\n",
    "print('Transforming images...')\n",
    "for root, folders, files in os.walk(training_folder_name):\n",
    "    for sub_folder in folders:\n",
    "        print('processing folder ' + sub_folder)\n",
    "        # Create a matching subfolder in the output dir\n",
    "        saveFolder = os.path.join(train_folder,sub_folder)\n",
    "        if not os.path.exists(saveFolder):\n",
    "            os.makedirs(saveFolder)\n",
    "        # Loop through the files in the subfolder\n",
    "        file_names = os.listdir(os.path.join(root,sub_folder))\n",
    "        for file_name in file_names:\n",
    "            # Open the file\n",
    "            file_path = os.path.join(root,sub_folder, file_name)\n",
    "            #print(\"reading \" + file_path)\n",
    "            image = Image.open(file_path)\n",
    "            # Create a resized version and save it\n",
    "            resized_image = resize_image(image, img_size)\n",
    "            saveAs = os.path.join(saveFolder, file_name)\n",
    "            #print(\"writing \" + saveAs)\n",
    "            resized_image.save(saveAs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_path):\n",
    "    # Load all the images\n",
    "    transformation = transforms.Compose([\n",
    "        # Randomly augment the image data\n",
    "            # Random horizontal flip\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "            # Random vertical flip\n",
    "        transforms.RandomVerticalFlip(0.3),\n",
    "        # transform to tensors\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize the pixel values (in R, G, and B channels)\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # Load all of the images, transforming them\n",
    "    full_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=transformation\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Split into training (70% and testing (30%) datasets)\n",
    "    train_size = int(0.7 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    \n",
    "    # use torch.utils.data.random_split for training/test split\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "    \n",
    "    # define a loader for the training data we can iterate through in 50-image batches\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=50,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # define a loader for the testing data we can iterate through in 50-image batches\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=50,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "        \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders ready to read ../working/data/natural_images\n"
     ]
    }
   ],
   "source": [
    "# Recall that we have resized the images and saved them into\n",
    "train_folder = '../working/data/natural_images'\n",
    "\n",
    "# Get the iterative dataloaders for test and training data\n",
    "train_loader, test_loader = load_dataset(train_folder)\n",
    "batch_size = train_loader.batch_size\n",
    "print(\"Data loaders ready to read\", train_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['square', 'triangle']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_folder_name = r\"C:\\Users\\neeraj.saini\\Desktop\\New folder\\Data\\shapes\"\n",
    "classes = sorted(os.listdir(training_folder_name))\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural net class\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    \n",
    "    # Defining the Constructor\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # In the init function, we define each layer we will use in our model\n",
    "        \n",
    "        # Our images are RGB, so we have input channels = 3. \n",
    "        # We will apply 12 filters in the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # A second convolutional layer takes 12 input channels, and generates 24 outputs\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # We in the end apply max pooling with a kernel size of 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # A drop layer deletes 20% of the features to help prevent overfitting\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.\n",
    "        # This means that our feature tensors are now 32 x 32, and we've generated 24 of them\n",
    "        \n",
    "        # We need to flatten these in order to feed them to a fully-connected layer\n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # In the forward function, pass the data through the layers we defined in the init function\n",
    "        \n",
    "        # Use a ReLU activation function after layer 1 (convolution 1 and pool)\n",
    "        x = F.relu(self.pool(self.conv1(x))) \n",
    "        \n",
    "        # Use a ReLU activation function after layer 2\n",
    "        x = F.relu(self.pool(self.conv2(x)))  \n",
    "        \n",
    "        # Select some features to drop to prevent overfitting (only drop during training)\n",
    "        x = F.dropout(self.drop(x), training=self.training)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 32 * 32 * 24)\n",
    "        # Feed to fully-connected layer to predict class\n",
    "        x = self.fc(x)\n",
    "        # Return class probabilities via a log_softmax function \n",
    "        return torch.log_softmax(x, dim=1)\n",
    "    \n",
    "device = \"cpu\"\n",
    "if (torch.cuda.is_available()):\n",
    "    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n",
    "    device = \"cuda\"\n",
    "\n",
    "# Create an instance of the model class and allocate it to the device\n",
    "model = Net(num_classes=len(classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_criteria = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128, 128])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_data[0][0].shape ########################################----------torch.Size([3, 128, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    print(\"Epoch:\", epoch)\n",
    "    # Process the images in batches\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Use the CPU or GPU as appropriate\n",
    "        # Recall that GPU is optimized for the operations we are dealing with\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Reset the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Push the data forward through the model layers\n",
    "        # print(data.shape)\n",
    "        output = model(data)\n",
    "        \n",
    "        # Get the loss\n",
    "        loss = loss_criteria(output, target)\n",
    "\n",
    "        # Keep a running total\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print metrics so we see some progress\n",
    "        # print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
    "    print('train loss:{:.6f}'.format(train_loss))        \n",
    "    # return average loss for the epoch\n",
    "    avg_loss = train_loss / (batch_idx+1)\n",
    "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
    "    model.eval()\n",
    "    loss_criteria = nn.CrossEntropyLoss()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        for data, target in test_loader:\n",
    "            batch_count += 1\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # print(data.shape)\n",
    "            # Get the predicted classes for this batch\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate the loss for this batch\n",
    "            test_loss += loss_criteria(output, target).item()\n",
    "            \n",
    "            # Calculate the accuracy for this batch\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += torch.sum(target==predicted).item()\n",
    "\n",
    "    # Calculate the average loss and total accuracy for this epoch\n",
    "    avg_loss = test_loss / batch_count\n",
    "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    # return average loss for the epoch\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n",
      "Epoch: 1\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([50, 3, 128, 128])\n",
      "torch.Size([39, 3, 128, 128])\n",
      "train loss:11.914850\n",
      "Training set: Average loss: 0.113475\n",
      "Validation set: Average loss: 0.000000, Accuracy: 2246/2246 (100%)\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n",
      "Epoch: 1\n",
      "train loss:19.704153\n",
      "Training set: Average loss: 0.187659\n",
      "Validation set: Average loss: 0.001161, Accuracy: 2245/2246 (100%)\n",
      "\n",
      "Epoch: 2\n",
      "train loss:1.791833\n",
      "Training set: Average loss: 0.017065\n",
      "Validation set: Average loss: 0.002591, Accuracy: 2245/2246 (100%)\n",
      "\n",
      "Epoch: 3\n",
      "train loss:1.206416\n",
      "Training set: Average loss: 0.011490\n",
      "Validation set: Average loss: 0.000711, Accuracy: 2245/2246 (100%)\n",
      "\n",
      "Epoch: 4\n",
      "train loss:0.436264\n",
      "Training set: Average loss: 0.004155\n",
      "Validation set: Average loss: 0.000104, Accuracy: 2246/2246 (100%)\n",
      "\n",
      "Epoch: 5\n",
      "train loss:0.553444\n",
      "Training set: Average loss: 0.005271\n",
      "Validation set: Average loss: 0.000006, Accuracy: 2246/2246 (100%)\n",
      "\n",
      "Epoch: 6\n",
      "train loss:0.982023\n",
      "Training set: Average loss: 0.009353\n",
      "Validation set: Average loss: 0.000001, Accuracy: 2246/2246 (100%)\n",
      "\n",
      "Epoch: 7\n",
      "train loss:0.497322\n",
      "Training set: Average loss: 0.004736\n",
      "Validation set: Average loss: 0.000001, Accuracy: 2246/2246 (100%)\n",
      "\n",
      "Epoch: 8\n",
      "train loss:0.187783\n",
      "Training set: Average loss: 0.001788\n",
      "Validation set: Average loss: 0.000001, Accuracy: 2246/2246 (100%)\n",
      "\n",
      "Epoch: 9\n",
      "train loss:0.645864\n",
      "Training set: Average loss: 0.006151\n",
      "Validation set: Average loss: 0.000005, Accuracy: 2246/2246 (100%)\n",
      "\n",
      "Epoch: 10\n",
      "train loss:4.356007\n",
      "Training set: Average loss: 0.041486\n",
      "Validation set: Average loss: 0.000023, Accuracy: 2246/2246 (100%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use an \"Adam\" optimizer to adjust weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Specify the loss criteria\n",
    "# loss_criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "# Track metrics in these arrays\n",
    "epoch_nums = []\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "# Train over 10 epochs (We restrict to 10 for time issues)\n",
    "epochs = 10\n",
    "print('Training on', device)\n",
    "for epoch in range(1, epochs + 1):\n",
    "        train_loss = train(model, device, train_loader, optimizer, epoch)\n",
    "        test_loss = test(model, device, test_loader)\n",
    "        epoch_nums.append(epoch)\n",
    "        training_loss.append(train_loss)\n",
    "        validation_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\model_square_tri.h5\"\n",
    "torch.save(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = r\"C:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\model_square_tri_script.h5\"\n",
    "# torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(r\"C:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\model_square_tri.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = test(model,'cpu', test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, target in test_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    img = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([46, 3, 128, 128])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = data[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-31.9006,   0.0000]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x236d8f2a130>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp = np.moveaxis(img.to('cpu').detach().numpy(), 0, 2)\n",
    "plt.imshow(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(r\"C:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\Triangles-05.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.moveaxis(img.to('cpu').detach().numpy(), 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "# Apply the transformation to convert PIL image to PyTorch tensor\n",
    "torch_tensor = to_tensor(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128, 128])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_image = resize_image(image, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neeraj.saini\\AppData\\Local\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\functional.py:1352: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
      "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6436, -0.2148]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
