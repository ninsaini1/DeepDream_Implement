{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neeraj.saini\\AppData\\Local\\anaconda3\\envs\\py38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import numbers\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural net class\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    \n",
    "    # Defining the Constructor\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # In the init function, we define each layer we will use in our model\n",
    "        \n",
    "        # Our images are RGB, so we have input channels = 3. \n",
    "        # We will apply 12 filters in the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # A second convolutional layer takes 12 input channels, and generates 24 outputs\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # We in the end apply max pooling with a kernel size of 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # A drop layer deletes 20% of the features to help prevent overfitting\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.\n",
    "        # This means that our feature tensors are now 32 x 32, and we've generated 24 of them\n",
    "        \n",
    "        # We need to flatten these in order to feed them to a fully-connected layer\n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # In the forward function, pass the data through the layers we defined in the init function\n",
    "        \n",
    "        # Use a ReLU activation function after layer 1 (convolution 1 and pool)\n",
    "        x = F.relu(self.pool(self.conv1(x))) \n",
    "        \n",
    "        # Use a ReLU activation function after layer 2\n",
    "        x = F.relu(self.pool(self.conv2(x)))  \n",
    "        \n",
    "        # Select some features to drop to prevent overfitting (only drop during training)\n",
    "        x = F.dropout(self.drop(x), training=self.training)\n",
    "        \n",
    "        # Flatten\n",
    "        # x = x.view(-1, 5400)\n",
    "        x = x.view(-1, 32 * 32 * 24)\n",
    "        # Feed to fully-connected layer to predict class\n",
    "        x = self.fc(x)\n",
    "        # Return class probabilities via a log_softmax function \n",
    "        return torch.log_softmax(x, dim=1)\n",
    "    \n",
    "device = \"cpu\"\n",
    "if (torch.cuda.is_available()):\n",
    "    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n",
    "    device = \"cuda\"\n",
    "\n",
    "# Create an instance of the model class and allocate it to the device\n",
    "# model = Net(num_classes=len(classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------#\n",
    "# Can't import torch saved model directly hence first intializing the model and then loading the parameters into it.\n",
    "#--------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\model_square_tri.h5\"\n",
    "model = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (drop): Dropout2d(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=24576, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_input_adapter(img):\n",
    "    # shape = (1, 3, H, W)\n",
    "    # tensor = transforms.ToTensor()(img).to('cpu').unsqueeze(0)\n",
    "    tensor = transforms.ToTensor()(img).to('cpu')\n",
    "    tensor.requires_grad = True  # we need to collect gradients for the input image\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def pytorch_output_adapter(tensor):\n",
    "    # Push to CPU, detach from the computational graph, convert from (1, 3, H, W) tensor into (H, W, 3) numpy image\n",
    "    return np.moveaxis(tensor.to('cpu').detach().numpy(), 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CascadeGaussianSmoothing(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply gaussian smoothing separately for each channel (depthwise convolution).\n",
    "\n",
    "    Arguments:\n",
    "        kernel_size (int, sequence): Size of the gaussian kernel.\n",
    "        sigma (float, sequence): Standard deviation of the gaussian kernel.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, sigma):\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(kernel_size, numbers.Number):\n",
    "            kernel_size = [kernel_size, kernel_size]\n",
    "\n",
    "        cascade_coefficients = [0.5, 1.0, 2.0]  # std multipliers, hardcoded to use 3 different Gaussian kernels\n",
    "        sigmas = [[coeff * sigma, coeff * sigma] for coeff in cascade_coefficients]  # isotropic Gaussian\n",
    "\n",
    "        self.pad = int(kernel_size[0] / 2)  # assure we have the same spatial resolution\n",
    "\n",
    "        # The gaussian kernel is the product of the gaussian function of each dimension.\n",
    "        kernels = []\n",
    "        meshgrids = torch.meshgrid([torch.arange(size, dtype=torch.float32) for size in kernel_size])\n",
    "        for sigma in sigmas:\n",
    "            kernel = torch.ones_like(meshgrids[0])\n",
    "            for size_1d, std_1d, grid in zip(kernel_size, sigma, meshgrids):\n",
    "                mean = (size_1d - 1) / 2\n",
    "                kernel *= 1 / (std_1d * math.sqrt(2 * math.pi)) * torch.exp(-((grid - mean) / std_1d) ** 2 / 2)\n",
    "            kernels.append(kernel)\n",
    "\n",
    "        gaussian_kernels = []\n",
    "        for kernel in kernels:\n",
    "            # Normalize - make sure sum of values in gaussian kernel equals 1.\n",
    "            kernel = kernel / torch.sum(kernel)\n",
    "            # Reshape to depthwise convolutional weight\n",
    "            kernel = kernel.view(1, 1, *kernel.shape)\n",
    "            kernel = kernel.repeat(3, 1, 1, 1)\n",
    "            kernel = kernel.to(device)\n",
    "\n",
    "            gaussian_kernels.append(kernel)\n",
    "\n",
    "        self.weight1 = gaussian_kernels[0]\n",
    "        self.weight2 = gaussian_kernels[1]\n",
    "        self.weight3 = gaussian_kernels[2]\n",
    "        self.conv = F.conv2d\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = F.pad(input, [self.pad, self.pad, self.pad, self.pad], mode='reflect')\n",
    "\n",
    "        # Apply Gaussian kernels depthwise over the input (hence groups equals the number of input channels)\n",
    "        # shape = (1, 3, H, W) -> (1, 3, H, W)\n",
    "        num_in_channels = input.shape[1]\n",
    "        grad1 = self.conv(input, weight=self.weight1, groups=num_in_channels)\n",
    "        grad2 = self.conv(input, weight=self.weight2, groups=num_in_channels)\n",
    "        grad3 = self.conv(input, weight=self.weight3, groups=num_in_channels)\n",
    "\n",
    "        return (grad1 + grad2 + grad3) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neuron_act(input_tensor):\n",
    "    # Dictionary to store activations for each layer\n",
    "    activations = {}\n",
    "    # Define a function to store the activations\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activations[name] = output\n",
    "        return hook\n",
    "\n",
    "    # Register hooks to store activations\n",
    "    target_layer = 1  # Choose the layer you want to examine\n",
    "    layer_name = f'conv{target_layer}'\n",
    "    model.conv1.register_forward_hook(get_activation(layer_name))\n",
    "\n",
    "    # Perform forward pass\n",
    "    # with torch.no_grad():\n",
    "    model.eval()\n",
    "    _ = model(input_tensor)\n",
    "    # print(\"Helooooooooooooooooooooooooooooo\")\n",
    "\n",
    "    # Access the stored activations for the chosen layer\n",
    "    if layer_name in activations:\n",
    "        activation_value = activations[layer_name]\n",
    "        return activation_value\n",
    "    else:\n",
    "        print(f'Layer {layer_name} activations not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOWER_IMAGE_BOUND = torch.tensor((-IMAGENET_MEAN_1 / IMAGENET_STD_1).reshape(1, -1, 1, 1)).to(DEVICE)\n",
    "# UPPER_IMAGE_BOUND = torch.tensor(((1 - IMAGENET_MEAN_1) / IMAGENET_STD_1).reshape(1, -1, 1, 1)).to(DEVICE)\n",
    "\n",
    "\n",
    "def gradient_ascent(model, input_tensor, layer_ids_to_use, iteration):\n",
    "    # Step 0: Feed forward pass\n",
    "    out = model(input_tensor)\n",
    "    # CHeck out \n",
    "    '''\n",
    "    y = input_tensor**2\n",
    "    loss = y.mean()\n",
    "    loss.backward()\n",
    "    print(input_tensor.grad.data) # This is working and giving the value of the gradients\n",
    "    '''\n",
    "    # Step 1: Grab activations/feature maps of interest\n",
    "    activations = get_neuron_act(input_tensor)\n",
    "    # input_tensor.retain_grad()\n",
    "    ################################33 Check grad\n",
    "    #print(activations[0].requires_grad) #############################----------------- Giving True\n",
    "\n",
    "    # Step 2: Calculate loss over activations\n",
    "    losses = []\n",
    "    for layer_activation in activations[0]:\n",
    "        '''\n",
    "        Use torch.norm(torch.flatten(layer_activation), p) with p=2 for L2 loss and p=1 for L1 loss. \n",
    "        But I'll use the MSE as it works really good, I didn't notice any serious change when going to L1/L2.\n",
    "        using torch.zeros_like as if we wanted to make activations as small as possible but we'll do gradient ascent\n",
    "        and that will cause it to actually amplify whatever the network \"sees\" thus yielding the famous DeepDream look\n",
    "        '''\n",
    "        loss_component = torch.nn.MSELoss(reduction='mean')(layer_activation, input_tensor) ###torch.zeros_like(layer_activation, requires_grad=True)\n",
    "        losses.append(loss_component)\n",
    "    # losses = torch.tensor(losses, requires_grad= True)\n",
    "    loss = torch.mean(torch.stack(losses))\n",
    "    # print(losses[0]) ########################################----tensor(0.1189, grad_fn=<MseLossBackward0>)\n",
    "    # print(len(losses)) #############################----12\n",
    "    # print(loss) ################################################----tensor(0.6373) ------tensor(0.6373, grad_fn=<MeanBackward0>)\n",
    "    # Added requers_grad = True in loss_component. Hence next step is not needed.\n",
    "    # loss.requires_grad = True # Was giving error \"element 0 of variables does not require grad and does not have a grad_fn\"\n",
    "    loss.backward()\n",
    "\n",
    "    # Step 3: Process image gradients (smoothing + normalization, more an art then a science)\n",
    "    grad = input_tensor.grad.data # Giving error nonetype object has no attribute data. Means there is some problem in loss.backward()\n",
    "    # print(grad.shape) ################################################------None ------torch.Size([12, 128, 128])\n",
    "    \n",
    "    # Applies 3 Gaussian kernels and thus \"blurs\" or smoothens the gradients and gives visually more pleasing results\n",
    "    # We'll see the details of this one in the next cell and that's all, you now understand DeepDream!\n",
    "    # sigma = ((iteration + 1) / 10) * 2.0 + 0.5\n",
    "    # smooth_grad = CascadeGaussianSmoothing(kernel_size=9, sigma=sigma)(grad)  # \"magic number\" 9 just works well\n",
    "\n",
    "    # Normalize the gradients (make them have mean = 0 and std = 1)\n",
    "    # I didn't notice any big difference normalizing the mean as well - feel free to experiment\n",
    "    g_std = torch.std(grad)\n",
    "    g_mean = torch.mean(grad)\n",
    "    smooth_grad = grad - g_mean\n",
    "    smooth_grad = grad / g_std\n",
    "    # smooth_grad = smooth_grad - g_mean\n",
    "    # smooth_grad = smooth_grad / g_std\n",
    "\n",
    "    # Step 4: Update image using the calculated gradients (gradient ascent step)\n",
    "    # print(input_tensor.data.shape)\n",
    "    # print(smooth_grad.shape)\n",
    "    input_tensor.data += 0.09 * smooth_grad\n",
    "\n",
    "    # Step 5: Clear gradients and clamp the data (otherwise values would explode to +- \"infinity\")\n",
    "    input_tensor.grad.data.zero_()\n",
    "    #input_tensor.data = torch.max(torch.min(input_tensor, 255), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_dream_static_image():\n",
    "    \n",
    "    img = np.random.uniform(low=0.0, high=1.0, size=[128, 128, 3]).astype(np.float32)\n",
    "    shape = img.shape\n",
    "    # img = pre_process_numpy_img(img)\n",
    "    original_shape = img.shape[:-1]  # save initial height and width  \n",
    "    for iteration in range(5000): ########## Giving more clear image at 5K. Crashing at 10K.\n",
    "        input_tensor = pytorch_input_adapter(img)  # convert to trainable tensor\n",
    "        # print(input_tensor.requires_grad)\n",
    "        gradient_ascent(model, input_tensor, ['conv1'], iteration)\n",
    "        #print(input_tensor.shape)\n",
    "        img = pytorch_output_adapter(input_tensor)\n",
    "    return img\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "img = deep_dream_static_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  406.70422 ,  -347.87036 ,  -344.84933 ],\n",
       "        [  380.93396 ,  -476.81946 ,  -369.7387  ],\n",
       "        [   90.81843 ,  -312.31415 ,  -124.41073 ],\n",
       "        ...,\n",
       "        [  205.78888 ,  -272.91647 ,  -161.27852 ],\n",
       "        [    8.63181 ,  -127.70345 ,    11.395815],\n",
       "        [  -31.049728,   -13.489065,    82.99378 ]],\n",
       "\n",
       "       [[ -675.10297 ,   596.4533  ,   477.43005 ],\n",
       "        [ -815.11554 ,  1109.0106  ,   777.89294 ],\n",
       "        [ -564.91565 ,  1221.6641  ,   713.6283  ],\n",
       "        ...,\n",
       "        [ -342.40335 ,   672.325   ,   375.50977 ],\n",
       "        [  -61.71721 ,   484.60358 ,   240.70494 ],\n",
       "        [   22.231123,   187.42734 ,    52.254265]],\n",
       "\n",
       "       [[  512.40717 ,  -368.02957 ,  -371.61823 ],\n",
       "        [  623.8247  , -1099.5259  ,  -866.298   ],\n",
       "        [  424.78494 , -1575.9645  , -1175.3018  ],\n",
       "        ...,\n",
       "        [   40.274994,  -526.94073 ,  -551.08813 ],\n",
       "        [ -110.64669 ,  -592.8635  ,  -558.0601  ],\n",
       "        [ -166.48653 ,  -300.7506  ,  -207.21577 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  -40.562237,    25.893417,    54.51629 ],\n",
       "        [  182.48459 ,    41.722313,    61.635975],\n",
       "        [  530.182   ,  -227.10481 ,  -106.53406 ],\n",
       "        ...,\n",
       "        [  -97.56464 ,   491.44867 ,   281.56396 ],\n",
       "        [  139.04288 ,   429.6391  ,   297.76862 ],\n",
       "        [  169.41359 ,   238.38843 ,   224.64272 ]],\n",
       "\n",
       "       [[   92.205475,   -95.164505,   -93.38452 ],\n",
       "        [    5.341238,  -193.66672 ,   -96.93402 ],\n",
       "        [ -130.40697 ,   -27.95361 ,    11.031861],\n",
       "        ...,\n",
       "        [ -186.43047 ,  -369.10687 ,  -395.50693 ],\n",
       "        [ -237.72046 ,  -341.2955  ,  -333.91724 ],\n",
       "        [ -208.31833 ,  -172.47665 ,  -195.38727 ]],\n",
       "\n",
       "       [[  -43.363537,    37.36752 ,    15.613703],\n",
       "        [  -27.108267,   188.71547 ,    52.18314 ],\n",
       "        [  -35.104702,   269.89014 ,    94.92563 ],\n",
       "        ...,\n",
       "        [  240.18309 ,   225.43758 ,   278.99442 ],\n",
       "        [  176.62485 ,   194.22104 ,   204.26003 ],\n",
       "        [  107.20873 ,   117.958664,   138.09254 ]]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_image = img / np.max(img)\n",
    "\n",
    "# Scale the normalized image to the integer range [0, 255]\n",
    "integer_image = (normalized_image * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[115,  25, 130],\n",
       "        [168, 238, 172],\n",
       "        [115,  25, 127],\n",
       "        ...,\n",
       "        [ 84,  24,  98],\n",
       "        [181, 243, 181],\n",
       "        [107,   8, 106]],\n",
       "\n",
       "       [[169,  75, 138],\n",
       "        [ 56, 136, 141],\n",
       "        [234, 100, 152],\n",
       "        ...,\n",
       "        [134,  68, 102],\n",
       "        [ 95, 158, 161],\n",
       "        [203,  61, 144]],\n",
       "\n",
       "       [[175,  75, 141],\n",
       "        [ 44, 124, 147],\n",
       "        [237,  96, 157],\n",
       "        ...,\n",
       "        [132,  62, 101],\n",
       "        [ 82, 146, 162],\n",
       "        [206,  61, 143]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[177,  78, 144],\n",
       "        [ 34, 121, 137],\n",
       "        [246, 102, 162],\n",
       "        ...,\n",
       "        [122,  64,  98],\n",
       "        [ 86, 145, 158],\n",
       "        [205,  61, 144]],\n",
       "\n",
       "       [[158,  96, 140],\n",
       "        [ 45,  92, 128],\n",
       "        [243, 128, 175],\n",
       "        ...,\n",
       "        [106,  74,  98],\n",
       "        [ 94, 126, 161],\n",
       "        [204,  78, 150]],\n",
       "\n",
       "       [[126, 114,  98],\n",
       "        [ 68,  75, 137],\n",
       "        [207, 152, 130],\n",
       "        ...,\n",
       "        [ 77,  74,  60],\n",
       "        [123, 116, 162],\n",
       "        [168, 103, 110]]], dtype=uint8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integer_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy array to a Pillow Image\n",
    "pillow_image = Image.fromarray(integer_image)\n",
    "\n",
    "# Save the image to a file (e.g., in PNG format)\n",
    "pillow_image.save(r\"C:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\conv1_10_5k.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.random.uniform(low=0.0, high=1.0, size=[128, 128, 3]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = pytorch_input_adapter(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128, 128])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.6012e+01, -1.1921e-07], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = model(input_tensor)\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neeraj.saini\\AppData\\Local\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\functional.py:1352: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
      "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n"
     ]
    }
   ],
   "source": [
    "activation_value = get_neuron_act(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 128, 128])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_value[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.MSELoss(reduction='mean')(activation_value, torch.zeros_like(activation_value, requires_grad=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6373, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(activation_value.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for x computed successfully!\n",
      "Gradients for x: tensor([6.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor and enable gradient tracking\n",
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "# Some computation\n",
    "y = x ** 2\n",
    "# Define a scalar loss\n",
    "loss = y.mean()\n",
    "\n",
    "# Perform backpropagation\n",
    "loss.backward()\n",
    "\n",
    "# Check if gradients are computed for the tensor\n",
    "if x.grad is not None:\n",
    "    print(\"Gradients for x computed successfully!\")\n",
    "    print(\"Gradients for x:\", x.grad)\n",
    "else:\n",
    "    print(\"No gradients computed for x. Ensure backpropagation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.], requires_grad=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
