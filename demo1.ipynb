{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neeraj.saini\\AppData\\Local\\anaconda3\\envs\\py38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import numbers\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural net class\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    \n",
    "    # Defining the Constructor\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # In the init function, we define each layer we will use in our model\n",
    "        \n",
    "        # Our images are RGB, so we have input channels = 3. \n",
    "        # We will apply 12 filters in the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # A second convolutional layer takes 12 input channels, and generates 24 outputs\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # We in the end apply max pooling with a kernel size of 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # A drop layer deletes 20% of the features to help prevent overfitting\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.\n",
    "        # This means that our feature tensors are now 32 x 32, and we've generated 24 of them\n",
    "        \n",
    "        # We need to flatten these in order to feed them to a fully-connected layer\n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # In the forward function, pass the data through the layers we defined in the init function\n",
    "        \n",
    "        # Use a ReLU activation function after layer 1 (convolution 1 and pool)\n",
    "        x = F.relu(self.pool(self.conv1(x))) \n",
    "        \n",
    "        # Use a ReLU activation function after layer 2\n",
    "        x = F.relu(self.pool(self.conv2(x)))  \n",
    "        \n",
    "        # Select some features to drop to prevent overfitting (only drop during training)\n",
    "        x = F.dropout(self.drop(x), training=self.training)\n",
    "        \n",
    "        # Flatten\n",
    "        # x = x.view(-1, 5400)\n",
    "        x = x.view(-1, 32 * 32 * 24)\n",
    "        # Feed to fully-connected layer to predict class\n",
    "        x = self.fc(x)\n",
    "        # Return class probabilities via a log_softmax function \n",
    "        return torch.log_softmax(x, dim=1)\n",
    "    \n",
    "device = \"cpu\"\n",
    "if (torch.cuda.is_available()):\n",
    "    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n",
    "    device = \"cuda\"\n",
    "\n",
    "# Create an instance of the model class and allocate it to the device\n",
    "# model = Net(num_classes=len(classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (drop): Dropout2d(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=24576, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r\"C:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\model_square_tri_script.h5\"\n",
    "#--------------------------------------------------------#\n",
    "# Can't import torch saved model directly hence first intializing the model and then loading the parameters into it.\n",
    "#--------------------------------------------------------#\n",
    "model = Net() \n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\model_square_tri.h5\"\n",
    "model = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (drop): Dropout2d(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=24576, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pre_process_numpy_img(img):\n",
    "#     assert isinstance(img, np.ndarray), f'Expected numpy image got {type(img)}'\n",
    "\n",
    "#     img = (img - IMAGENET_MEAN_1) / IMAGENET_STD_1  # normalize image\n",
    "#     return img\n",
    "\n",
    "\n",
    "# def post_process_numpy_img(img):\n",
    "#     assert isinstance(img, np.ndarray), f'Expected numpy image got {type(img)}'\n",
    "\n",
    "#     if img.shape[0] == 3:  # if channel-first format move to channel-last (CHW -> HWC)\n",
    "#         img = np.moveaxis(img, 0, 2)\n",
    "\n",
    "#     mean = IMAGENET_MEAN_1.reshape(1, 1, -1)\n",
    "#     std = IMAGENET_STD_1.reshape(1, 1, -1)\n",
    "#     img = (img * std) + mean  # de-normalize\n",
    "#     img = np.clip(img, 0., 1.)  # make sure it's in the [0, 1] range\n",
    "\n",
    "#     return img\n",
    "\n",
    "\n",
    "def pytorch_input_adapter(img):\n",
    "    # shape = (1, 3, H, W)\n",
    "    # tensor = transforms.ToTensor()(img).to('cpu').unsqueeze(0)\n",
    "    tensor = transforms.ToTensor()(img).to('cpu')\n",
    "    tensor.requires_grad = True  # we need to collect gradients for the input image\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def pytorch_output_adapter(tensor):\n",
    "    # Push to CPU, detach from the computational graph, convert from (1, 3, H, W) tensor into (H, W, 3) numpy image\n",
    "    return np.moveaxis(tensor.to('cpu').detach().numpy()[0], 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CascadeGaussianSmoothing(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply gaussian smoothing separately for each channel (depthwise convolution).\n",
    "\n",
    "    Arguments:\n",
    "        kernel_size (int, sequence): Size of the gaussian kernel.\n",
    "        sigma (float, sequence): Standard deviation of the gaussian kernel.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, sigma):\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(kernel_size, numbers.Number):\n",
    "            kernel_size = [kernel_size, kernel_size]\n",
    "\n",
    "        cascade_coefficients = [0.5, 1.0, 2.0]  # std multipliers, hardcoded to use 3 different Gaussian kernels\n",
    "        sigmas = [[coeff * sigma, coeff * sigma] for coeff in cascade_coefficients]  # isotropic Gaussian\n",
    "\n",
    "        self.pad = int(kernel_size[0] / 2)  # assure we have the same spatial resolution\n",
    "\n",
    "        # The gaussian kernel is the product of the gaussian function of each dimension.\n",
    "        kernels = []\n",
    "        meshgrids = torch.meshgrid([torch.arange(size, dtype=torch.float32) for size in kernel_size])\n",
    "        for sigma in sigmas:\n",
    "            kernel = torch.ones_like(meshgrids[0])\n",
    "            for size_1d, std_1d, grid in zip(kernel_size, sigma, meshgrids):\n",
    "                mean = (size_1d - 1) / 2\n",
    "                kernel *= 1 / (std_1d * math.sqrt(2 * math.pi)) * torch.exp(-((grid - mean) / std_1d) ** 2 / 2)\n",
    "            kernels.append(kernel)\n",
    "\n",
    "        gaussian_kernels = []\n",
    "        for kernel in kernels:\n",
    "            # Normalize - make sure sum of values in gaussian kernel equals 1.\n",
    "            kernel = kernel / torch.sum(kernel)\n",
    "            # Reshape to depthwise convolutional weight\n",
    "            kernel = kernel.view(1, 1, *kernel.shape)\n",
    "            kernel = kernel.repeat(3, 1, 1, 1)\n",
    "            kernel = kernel.to(device)\n",
    "\n",
    "            gaussian_kernels.append(kernel)\n",
    "\n",
    "        self.weight1 = gaussian_kernels[0]\n",
    "        self.weight2 = gaussian_kernels[1]\n",
    "        self.weight3 = gaussian_kernels[2]\n",
    "        self.conv = F.conv2d\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = F.pad(input, [self.pad, self.pad, self.pad, self.pad], mode='reflect')\n",
    "\n",
    "        # Apply Gaussian kernels depthwise over the input (hence groups equals the number of input channels)\n",
    "        # shape = (1, 3, H, W) -> (1, 3, H, W)\n",
    "        num_in_channels = input.shape[1]\n",
    "        grad1 = self.conv(input, weight=self.weight1, groups=num_in_channels)\n",
    "        grad2 = self.conv(input, weight=self.weight2, groups=num_in_channels)\n",
    "        grad3 = self.conv(input, weight=self.weight3, groups=num_in_channels)\n",
    "\n",
    "        return (grad1 + grad2 + grad3) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOWER_IMAGE_BOUND = torch.tensor((-IMAGENET_MEAN_1 / IMAGENET_STD_1).reshape(1, -1, 1, 1)).to(DEVICE)\n",
    "# UPPER_IMAGE_BOUND = torch.tensor(((1 - IMAGENET_MEAN_1) / IMAGENET_STD_1).reshape(1, -1, 1, 1)).to(DEVICE)\n",
    "\n",
    "\n",
    "def gradient_ascent(model, input_tensor, layer_ids_to_use, iteration):\n",
    "    # Step 0: Feed forward pass\n",
    "    out = model(input_tensor)\n",
    "    # CHeck out \n",
    "    '''\n",
    "    y = input_tensor**2\n",
    "    loss = y.mean()\n",
    "    loss.backward()\n",
    "    print(input_tensor.grad.data) # This is working and giving the value of the gradients\n",
    "    '''\n",
    "    # Step 1: Grab activations/feature maps of interest\n",
    "    activations = get_neuron_act()\n",
    "    input_tensor.retain_grad()\n",
    "    ################################33 Check grad\n",
    "    #print(activations[0].requires_grad) #############################----------------- Giving True\n",
    "\n",
    "    # Step 2: Calculate loss over activations\n",
    "    losses = []\n",
    "    for layer_activation in activations:\n",
    "        '''\n",
    "        Use torch.norm(torch.flatten(layer_activation), p) with p=2 for L2 loss and p=1 for L1 loss. \n",
    "        But I'll use the MSE as it works really good, I didn't notice any serious change when going to L1/L2.\n",
    "        using torch.zeros_like as if we wanted to make activations as small as possible but we'll do gradient ascent\n",
    "        and that will cause it to actually amplify whatever the network \"sees\" thus yielding the famous DeepDream look\n",
    "        '''\n",
    "        loss_component = torch.nn.MSELoss(reduction='mean')(layer_activation, torch.zeros_like(layer_activation, requires_grad=True))\n",
    "        losses.append(loss_component)\n",
    "    # losses = torch.tensor(losses, requires_grad= True)\n",
    "    loss = torch.mean(torch.stack(losses))\n",
    "    # print(losses[0]) ########################################----tensor(0.1189, grad_fn=<MseLossBackward0>)\n",
    "    # print(len(losses)) #############################----12\n",
    "    # print(loss) ################################################----tensor(0.6373) ------tensor(0.6373, grad_fn=<MeanBackward0>)\n",
    "    # Added requers_grad = True in loss_component. Hence next step is not needed.\n",
    "    # loss.requires_grad = True # Was giving error \"element 0 of variables does not require grad and does not have a grad_fn\"\n",
    "    loss.backward()\n",
    "\n",
    "    # Step 3: Process image gradients (smoothing + normalization, more an art then a science)\n",
    "    grad = input_tensor.grad.data # Giving error nonetype object has no attribute data. Means there is some problem in loss.backward()\n",
    "    # print(grad.shape) ################################################------None ------torch.Size([12, 128, 128])\n",
    "    \n",
    "    # Applies 3 Gaussian kernels and thus \"blurs\" or smoothens the gradients and gives visually more pleasing results\n",
    "    # We'll see the details of this one in the next cell and that's all, you now understand DeepDream!\n",
    "    # sigma = ((iteration + 1) / 10) * 2.0 + 0.5\n",
    "    # smooth_grad = CascadeGaussianSmoothing(kernel_size=9, sigma=sigma)(grad)  # \"magic number\" 9 just works well\n",
    "\n",
    "    # Normalize the gradients (make them have mean = 0 and std = 1)\n",
    "    # I didn't notice any big difference normalizing the mean as well - feel free to experiment\n",
    "    g_std = torch.std(grad)\n",
    "    g_mean = torch.mean(grad)\n",
    "    smooth_grad = grad - g_mean\n",
    "    smooth_grad = grad / g_std\n",
    "    # smooth_grad = smooth_grad - g_mean\n",
    "    # smooth_grad = smooth_grad / g_std\n",
    "\n",
    "    # Step 4: Update image using the calculated gradients (gradient ascent step)\n",
    "    print(input_tensor.data.shape)\n",
    "    print(smooth_grad.shape)\n",
    "    input_tensor.data += 0.09 * smooth_grad\n",
    "\n",
    "    # Step 5: Clear gradients and clamp the data (otherwise values would explode to +- \"infinity\")\n",
    "    input_tensor.grad.data.zero_()\n",
    "    # input_tensor.data = torch.max(torch.min(input_tensor, UPPER_IMAGE_BOUND), LOWER_IMAGE_BOUND)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_dream_static_image():\n",
    "    layer_ids_to_use = 'layer_0'\n",
    "    \n",
    "    img = np.random.uniform(low=0.0, high=1.0, size=[128, 128, 3]).astype(np.float32)\n",
    "    shape = img.shape\n",
    "    # img = pre_process_numpy_img(img)\n",
    "    original_shape = img.shape[:-1]  # save initial height and width  \n",
    "    for iteration in range(10):\n",
    "        input_tensor = pytorch_input_adapter(img)  # convert to trainable tensor\n",
    "        # print(input_tensor.requires_grad)\n",
    "        gradient_ascent(model, input_tensor, ['conv1'], iteration)\n",
    "        img = pytorch_output_adapter(input_tensor)\n",
    "    return img\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neeraj.saini\\AppData\\Local\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\functional.py:1352: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
      "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\demo1.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m img \u001b[39m=\u001b[39m deep_dream_static_image()\n",
      "\u001b[1;32mc:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\demo1.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     input_tensor \u001b[39m=\u001b[39m pytorch_input_adapter(img)  \u001b[39m# convert to trainable tensor\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# print(input_tensor.requires_grad)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     gradient_ascent(model, input_tensor, [\u001b[39m'\u001b[39;49m\u001b[39mconv1\u001b[39;49m\u001b[39m'\u001b[39;49m], iteration)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     img \u001b[39m=\u001b[39m pytorch_output_adapter(input_tensor)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mreturn\u001b[39;00m img\n",
      "\u001b[1;32mc:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\demo1.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# Step 3: Process image gradients (smoothing + normalization, more an art then a science)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m grad \u001b[39m=\u001b[39m input_tensor\u001b[39m.\u001b[39;49mgrad\u001b[39m.\u001b[39;49mdata \u001b[39m# Giving error nonetype object has no attribute data. Means there is some problem in loss.backward()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# print(grad.shape) ################################################------None ------torch.Size([12, 128, 128])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# Applies 3 Gaussian kernels and thus \"blurs\" or smoothens the gradients and gives visually more pleasing results\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# Normalize the gradients (make them have mean = 0 and std = 1)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# I didn't notice any big difference normalizing the mean as well - feel free to experiment\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m g_std \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstd(grad)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "img = deep_dream_static_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.random.uniform(low=0.0, high=1.0, size=[128, 128, 3]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = pytorch_input_adapter(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128, 128])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.6012e+01, -1.1921e-07], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = model(input_tensor)\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced time form 60 min to some seconds.\n",
    "def get_neuron_acts_layer( input_tensor, layer):\n",
    "    # getting activation one layer at a time\n",
    "    # Return activation corresponding to each neuron in a layer having dimension eaual to number of tokens\n",
    "    activations = {}\n",
    "\n",
    "    def caching_hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    \n",
    "    model.conv1.register_forward_hook(caching_hook)\n",
    "    out = model(input_tensor)\n",
    "\n",
    "    # model.run_with_hooks(\n",
    "    #     fwd_hooks=[(f\"blocks.{layer}.mlp.hook_post\", caching_hook)]\n",
    "    # )\n",
    "    return activation\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_neuron_acts_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\demo1.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m get_neuron_acts_layer(input_tensor, \u001b[39m'\u001b[39m\u001b[39mconv1\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_neuron_acts_layer' is not defined"
     ]
    }
   ],
   "source": [
    "get_neuron_acts_layer(input_tensor, 'conv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neuron_act():\n",
    "    # Dictionary to store activations for each layer\n",
    "    activations = {}\n",
    "    # Define a function to store the activations\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activations[name] = output\n",
    "        return hook\n",
    "\n",
    "    # Register hooks to store activations\n",
    "    target_layer = 1  # Choose the layer you want to examine\n",
    "    layer_name = f'conv{target_layer}'\n",
    "    model.conv1.register_forward_hook(get_activation(layer_name))\n",
    "\n",
    "    # Perform forward pass\n",
    "    # with torch.no_grad():\n",
    "    model.eval()\n",
    "    _ = model(input_tensor)\n",
    "    # print(\"Helooooooooooooooooooooooooooooo\")\n",
    "\n",
    "    # Access the stored activations for the chosen layer\n",
    "    if layer_name in activations:\n",
    "        activation_value = activations[layer_name]\n",
    "        return activation_value\n",
    "    else:\n",
    "        print(f'Layer {layer_name} activations not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neeraj.saini\\AppData\\Local\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\functional.py:1352: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
      "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n"
     ]
    }
   ],
   "source": [
    "activation_value = get_neuron_act()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 128, 128])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6695, -0.6265, -0.5858,  ..., -0.7480, -0.9726, -1.0012],\n",
       "         [-0.2610, -0.1446, -0.3553,  ..., -0.1961, -0.1654, -0.8382],\n",
       "         [ 0.2470, -0.4193, -0.1895,  ..., -0.2782, -0.0530, -0.6959],\n",
       "         ...,\n",
       "         [ 0.1820, -0.2008, -0.2226,  ..., -0.2668, -0.4181, -0.4064],\n",
       "         [-0.1095, -0.2266, -0.1430,  ..., -0.3595, -0.1630, -0.2087],\n",
       "         [ 0.4044, -0.0443,  0.0558,  ..., -0.1087, -0.0520, -0.2778]],\n",
       "\n",
       "        [[-0.4493, -0.8685, -0.5700,  ..., -0.8235, -0.9999, -0.9050],\n",
       "         [-0.3543, -0.9020, -0.7994,  ..., -0.6032, -0.9771, -1.1502],\n",
       "         [-0.2531, -0.8904, -0.6243,  ..., -0.8840, -0.7428, -0.9341],\n",
       "         ...,\n",
       "         [-0.1799, -0.9302, -0.9505,  ..., -0.6827, -0.8965, -0.7619],\n",
       "         [-0.2002, -0.4934, -0.9598,  ..., -0.6428, -0.8495, -0.7937],\n",
       "         [-0.0370, -0.4286, -0.1166,  ..., -0.2972, -0.4070, -0.3211]],\n",
       "\n",
       "        [[ 0.3560,  0.5160,  0.4111,  ...,  0.6373,  0.4670,  0.1578],\n",
       "         [ 0.9134,  0.7360,  0.6699,  ...,  1.2290,  0.8961,  0.4730],\n",
       "         [ 0.5837,  0.6599,  0.8617,  ...,  0.9320,  0.9621,  0.4076],\n",
       "         ...,\n",
       "         [ 0.7671,  0.9701,  1.2964,  ...,  0.6123,  0.8180,  0.4786],\n",
       "         [ 0.7077,  1.0266,  0.5993,  ...,  0.7234,  0.8937,  0.1647],\n",
       "         [ 0.3746,  0.5951,  1.0748,  ...,  0.8454,  0.6997,  0.5134]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.2715,  0.2166, -0.0351,  ..., -0.0385,  0.1756,  0.0300],\n",
       "         [-0.2243, -0.3645, -0.0658,  ..., -0.2475, -0.2331, -0.3706],\n",
       "         [-0.1251, -0.1765, -0.2409,  ..., -0.2489, -0.1990, -0.4279],\n",
       "         ...,\n",
       "         [-0.1201, -0.0296, -0.0930,  ...,  0.0180, -0.1546, -0.2923],\n",
       "         [-0.1885, -0.3986, -0.0766,  ..., -0.1752, -0.4172, -0.1465],\n",
       "         [-0.1291, -0.0052, -0.1998,  ..., -0.3264, -0.2082, -0.0865]],\n",
       "\n",
       "        [[-0.2979, -1.0044, -0.6828,  ..., -0.7671, -0.9330, -1.1030],\n",
       "         [-0.4198, -1.2326, -0.9025,  ..., -0.8993, -0.9437, -0.7736],\n",
       "         [-0.4231, -0.6083, -0.9474,  ..., -0.9933, -1.2303, -0.8858],\n",
       "         ...,\n",
       "         [-0.4966, -0.7995, -0.9332,  ..., -0.9023, -0.9475, -0.7071],\n",
       "         [-0.5781, -1.1194, -1.0904,  ..., -1.2412, -0.9776, -0.6543],\n",
       "         [-0.1726, -0.1879, -0.3167,  ..., -0.1241, -0.5025, -0.1011]],\n",
       "\n",
       "        [[-0.0556,  0.3013,  0.1058,  ...,  0.3500,  0.0709, -0.0727],\n",
       "         [ 0.0411, -0.3772,  0.2178,  ..., -0.2244, -0.1532, -0.2287],\n",
       "         [-0.1555, -0.1169, -0.1686,  ..., -0.0748, -0.2727, -0.2539],\n",
       "         ...,\n",
       "         [-0.2634, -0.1939, -0.2400,  ..., -0.2439,  0.1757, -0.3881],\n",
       "         [-0.1969, -0.1941, -0.0500,  ..., -0.1007,  0.0959, -0.3350],\n",
       "         [-0.3254, -0.4338, -0.4925,  ..., -0.2480, -0.3403, -0.4614]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.MSELoss(reduction='mean')(activation_value, torch.zeros_like(activation_value, requires_grad=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6373, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(activation_value.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for x computed successfully!\n",
      "Gradients for x: tensor([6.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor and enable gradient tracking\n",
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "# Some computation\n",
    "y = x ** 2\n",
    "# Define a scalar loss\n",
    "loss = y.mean()\n",
    "\n",
    "# Perform backpropagation\n",
    "loss.backward()\n",
    "\n",
    "# Check if gradients are computed for the tensor\n",
    "if x.grad is not None:\n",
    "    print(\"Gradients for x computed successfully!\")\n",
    "    print(\"Gradients for x:\", x.grad)\n",
    "else:\n",
    "    print(\"No gradients computed for x. Ensure backpropagation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.], requires_grad=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
