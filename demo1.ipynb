{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neeraj.saini\\AppData\\Local\\anaconda3\\envs\\py38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import numbers\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural net class\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    \n",
    "    # Defining the Constructor\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # In the init function, we define each layer we will use in our model\n",
    "        \n",
    "        # Our images are RGB, so we have input channels = 3. \n",
    "        # We will apply 12 filters in the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # A second convolutional layer takes 12 input channels, and generates 24 outputs\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # We in the end apply max pooling with a kernel size of 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # A drop layer deletes 20% of the features to help prevent overfitting\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.\n",
    "        # This means that our feature tensors are now 32 x 32, and we've generated 24 of them\n",
    "        \n",
    "        # We need to flatten these in order to feed them to a fully-connected layer\n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # In the forward function, pass the data through the layers we defined in the init function\n",
    "        \n",
    "        # Use a ReLU activation function after layer 1 (convolution 1 and pool)\n",
    "        x = F.relu(self.pool(self.conv1(x))) \n",
    "        \n",
    "        # Use a ReLU activation function after layer 2\n",
    "        x = F.relu(self.pool(self.conv2(x)))  \n",
    "        \n",
    "        # Select some features to drop to prevent overfitting (only drop during training)\n",
    "        x = F.dropout(self.drop(x), training=self.training)\n",
    "        \n",
    "        # Flatten\n",
    "        # x = x.view(-1, 5400)\n",
    "        x = x.view(-1, 32 * 32 * 24)\n",
    "        # Feed to fully-connected layer to predict class\n",
    "        x = self.fc(x)\n",
    "        # Return class probabilities via a log_softmax function \n",
    "        return torch.log_softmax(x, dim=1)\n",
    "    \n",
    "device = \"cpu\"\n",
    "if (torch.cuda.is_available()):\n",
    "    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n",
    "    device = \"cuda\"\n",
    "\n",
    "# Create an instance of the model class and allocate it to the device\n",
    "# model = Net(num_classes=len(classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (drop): Dropout2d(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=24576, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r\"C:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\model_square_tri_script.h5\"\n",
    "#--------------------------------------------------------#\n",
    "# Can't import torch saved model directly hence first intializing the model and then loading the parameters into it.\n",
    "#--------------------------------------------------------#\n",
    "model = Net() \n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\model_square_tri.h5\"\n",
    "model = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (drop): Dropout2d(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=24576, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pre_process_numpy_img(img):\n",
    "#     assert isinstance(img, np.ndarray), f'Expected numpy image got {type(img)}'\n",
    "\n",
    "#     img = (img - IMAGENET_MEAN_1) / IMAGENET_STD_1  # normalize image\n",
    "#     return img\n",
    "\n",
    "\n",
    "# def post_process_numpy_img(img):\n",
    "#     assert isinstance(img, np.ndarray), f'Expected numpy image got {type(img)}'\n",
    "\n",
    "#     if img.shape[0] == 3:  # if channel-first format move to channel-last (CHW -> HWC)\n",
    "#         img = np.moveaxis(img, 0, 2)\n",
    "\n",
    "#     mean = IMAGENET_MEAN_1.reshape(1, 1, -1)\n",
    "#     std = IMAGENET_STD_1.reshape(1, 1, -1)\n",
    "#     img = (img * std) + mean  # de-normalize\n",
    "#     img = np.clip(img, 0., 1.)  # make sure it's in the [0, 1] range\n",
    "\n",
    "#     return img\n",
    "\n",
    "\n",
    "def pytorch_input_adapter(img):\n",
    "    # shape = (1, 3, H, W)\n",
    "    # tensor = transforms.ToTensor()(img).to('cpu').unsqueeze(0)\n",
    "    tensor = transforms.ToTensor()(img).to('cpu')\n",
    "    tensor.requires_grad = True  # we need to collect gradients for the input image\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def pytorch_output_adapter(tensor):\n",
    "    # Push to CPU, detach from the computational graph, convert from (1, 3, H, W) tensor into (H, W, 3) numpy image\n",
    "    return np.moveaxis(tensor.to('cpu').detach().numpy()[0], 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CascadeGaussianSmoothing(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply gaussian smoothing separately for each channel (depthwise convolution).\n",
    "\n",
    "    Arguments:\n",
    "        kernel_size (int, sequence): Size of the gaussian kernel.\n",
    "        sigma (float, sequence): Standard deviation of the gaussian kernel.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, sigma):\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(kernel_size, numbers.Number):\n",
    "            kernel_size = [kernel_size, kernel_size]\n",
    "\n",
    "        cascade_coefficients = [0.5, 1.0, 2.0]  # std multipliers, hardcoded to use 3 different Gaussian kernels\n",
    "        sigmas = [[coeff * sigma, coeff * sigma] for coeff in cascade_coefficients]  # isotropic Gaussian\n",
    "\n",
    "        self.pad = int(kernel_size[0] / 2)  # assure we have the same spatial resolution\n",
    "\n",
    "        # The gaussian kernel is the product of the gaussian function of each dimension.\n",
    "        kernels = []\n",
    "        meshgrids = torch.meshgrid([torch.arange(size, dtype=torch.float32) for size in kernel_size])\n",
    "        for sigma in sigmas:\n",
    "            kernel = torch.ones_like(meshgrids[0])\n",
    "            for size_1d, std_1d, grid in zip(kernel_size, sigma, meshgrids):\n",
    "                mean = (size_1d - 1) / 2\n",
    "                kernel *= 1 / (std_1d * math.sqrt(2 * math.pi)) * torch.exp(-((grid - mean) / std_1d) ** 2 / 2)\n",
    "            kernels.append(kernel)\n",
    "\n",
    "        gaussian_kernels = []\n",
    "        for kernel in kernels:\n",
    "            # Normalize - make sure sum of values in gaussian kernel equals 1.\n",
    "            kernel = kernel / torch.sum(kernel)\n",
    "            # Reshape to depthwise convolutional weight\n",
    "            kernel = kernel.view(1, 1, *kernel.shape)\n",
    "            kernel = kernel.repeat(3, 1, 1, 1)\n",
    "            kernel = kernel.to(DEVICE)\n",
    "\n",
    "            gaussian_kernels.append(kernel)\n",
    "\n",
    "        self.weight1 = gaussian_kernels[0]\n",
    "        self.weight2 = gaussian_kernels[1]\n",
    "        self.weight3 = gaussian_kernels[2]\n",
    "        self.conv = F.conv2d\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = F.pad(input, [self.pad, self.pad, self.pad, self.pad], mode='reflect')\n",
    "\n",
    "        # Apply Gaussian kernels depthwise over the input (hence groups equals the number of input channels)\n",
    "        # shape = (1, 3, H, W) -> (1, 3, H, W)\n",
    "        num_in_channels = input.shape[1]\n",
    "        grad1 = self.conv(input, weight=self.weight1, groups=num_in_channels)\n",
    "        grad2 = self.conv(input, weight=self.weight2, groups=num_in_channels)\n",
    "        grad3 = self.conv(input, weight=self.weight3, groups=num_in_channels)\n",
    "\n",
    "        return (grad1 + grad2 + grad3) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOWER_IMAGE_BOUND = torch.tensor((-IMAGENET_MEAN_1 / IMAGENET_STD_1).reshape(1, -1, 1, 1)).to(DEVICE)\n",
    "# UPPER_IMAGE_BOUND = torch.tensor(((1 - IMAGENET_MEAN_1) / IMAGENET_STD_1).reshape(1, -1, 1, 1)).to(DEVICE)\n",
    "\n",
    "\n",
    "def gradient_ascent(model, input_tensor, layer_ids_to_use, iteration):\n",
    "    # Step 0: Feed forward pass\n",
    "    out = model(input_tensor)\n",
    "\n",
    "    # Step 1: Grab activations/feature maps of interest\n",
    "    activations = [out[layer_id_to_use] for layer_id_to_use in layer_ids_to_use]\n",
    "\n",
    "    # Step 2: Calculate loss over activations\n",
    "    losses = []\n",
    "    for layer_activation in activations:\n",
    "        # Use torch.norm(torch.flatten(layer_activation), p) with p=2 for L2 loss and p=1 for L1 loss. \n",
    "        # But I'll use the MSE as it works really good, I didn't notice any serious change when going to L1/L2.\n",
    "        # using torch.zeros_like as if we wanted to make activations as small as possible but we'll do gradient ascent\n",
    "        # and that will cause it to actually amplify whatever the network \"sees\" thus yielding the famous DeepDream look\n",
    "        loss_component = torch.nn.MSELoss(reduction='mean')(layer_activation, torch.zeros_like(layer_activation))\n",
    "        losses.append(loss_component)\n",
    "\n",
    "    loss = torch.mean(torch.stack(losses))\n",
    "    loss.backward()\n",
    "\n",
    "    # Step 3: Process image gradients (smoothing + normalization, more an art then a science)\n",
    "    grad = input_tensor.grad.data\n",
    "\n",
    "    # Applies 3 Gaussian kernels and thus \"blurs\" or smoothens the gradients and gives visually more pleasing results\n",
    "    # We'll see the details of this one in the next cell and that's all, you now understand DeepDream!\n",
    "    sigma = ((iteration + 1) / 10) * 2.0 + 0.5\n",
    "    smooth_grad = CascadeGaussianSmoothing(kernel_size=9, sigma=sigma)(grad)  # \"magic number\" 9 just works well\n",
    "\n",
    "    # Normalize the gradients (make them have mean = 0 and std = 1)\n",
    "    # I didn't notice any big difference normalizing the mean as well - feel free to experiment\n",
    "    g_std = torch.std(smooth_grad)\n",
    "    g_mean = torch.mean(smooth_grad)\n",
    "    smooth_grad = smooth_grad - g_mean\n",
    "    smooth_grad = smooth_grad / g_std\n",
    "\n",
    "    # Step 4: Update image using the calculated gradients (gradient ascent step)\n",
    "    input_tensor.data += 0.09 * smooth_grad\n",
    "\n",
    "    # Step 5: Clear gradients and clamp the data (otherwise values would explode to +- \"infinity\")\n",
    "    input_tensor.grad.data.zero_()\n",
    "    # input_tensor.data = torch.max(torch.min(input_tensor, UPPER_IMAGE_BOUND), LOWER_IMAGE_BOUND)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_dream_static_image():\n",
    "    layer_ids_to_use = 'layer_0'\n",
    "    \n",
    "    img = np.random.uniform(low=0.0, high=1.0, size=[128, 128, 3]).astype(np.float32)\n",
    "    shape = img.shape\n",
    "    # img = pre_process_numpy_img(img)\n",
    "    original_shape = img.shape[:-1]  # save initial height and width  \n",
    "    for iteration in range(10):\n",
    "        input_tensor = pytorch_input_adapter(img)  # convert to trainable tensor\n",
    "        gradient_ascent(model, input_tensor, ['conv1'], iteration)\n",
    "        img = pytorch_output_adapter(input_tensor)\n",
    "    return img\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\demo1.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m img \u001b[39m=\u001b[39m deep_dream_static_image()\n",
      "\u001b[1;32mc:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\demo1.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m iteration \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     input_tensor \u001b[39m=\u001b[39m pytorch_input_adapter(img)  \u001b[39m# convert to trainable tensor\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     gradient_ascent(model, input_tensor, [\u001b[39m'\u001b[39;49m\u001b[39mconv1\u001b[39;49m\u001b[39m'\u001b[39;49m], iteration)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     img \u001b[39m=\u001b[39m pytorch_output_adapter(input_tensor)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mreturn\u001b[39;00m img\n",
      "\u001b[1;32mc:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\demo1.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m out \u001b[39m=\u001b[39m model(input_tensor)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Step 1: Grab activations/feature maps of interest\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m activations \u001b[39m=\u001b[39m [out[layer_id_to_use] \u001b[39mfor\u001b[39;00m layer_id_to_use \u001b[39min\u001b[39;00m layer_ids_to_use]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Step 2: Calculate loss over activations\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m losses \u001b[39m=\u001b[39m []\n",
      "\u001b[1;32mc:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\demo1.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m out \u001b[39m=\u001b[39m model(input_tensor)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Step 1: Grab activations/feature maps of interest\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m activations \u001b[39m=\u001b[39m [out[layer_id_to_use] \u001b[39mfor\u001b[39;00m layer_id_to_use \u001b[39min\u001b[39;00m layer_ids_to_use]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Step 2: Calculate loss over activations\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m losses \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "img = deep_dream_static_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.random.uniform(low=0.0, high=1.0, size=[128, 128, 3]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = pytorch_input_adapter(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128, 128])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.6012e+01, -1.1921e-07], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = model(input_tensor)\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced time form 60 min to some seconds.\n",
    "def get_neuron_acts_layer( input_tensor, layer):\n",
    "    # getting activation one layer at a time\n",
    "    # Return activation corresponding to each neuron in a layer having dimension eaual to number of tokens\n",
    "    cache = {}\n",
    "    activation = []\n",
    "\n",
    "    def caching_hook(input, output):\n",
    "        cache[\"act\"] = output\n",
    "        # temp = []\n",
    "        # for j in range(len(cache[\"act\"][0][0])):\n",
    "        #     temp.append(cache[\"act\"][0,:,j].tolist())\n",
    "        # temp = np.array(temp).squeeze()\n",
    "        # print(temp.shape)\n",
    "        print(output.shape)\n",
    "        activation.append(output)\n",
    "    \n",
    "    \n",
    "    model.fc.register_forward_hook(caching_hook)\n",
    "    out = model(input_tensor)\n",
    "\n",
    "    # model.run_with_hooks(\n",
    "    #     fwd_hooks=[(f\"blocks.{layer}.mlp.hook_post\", caching_hook)]\n",
    "    # )\n",
    "    return activation\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "caching_hook() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\demo1.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m get_neuron_acts_layer(input_tensor, \u001b[39m'\u001b[39;49m\u001b[39mconv1\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\demo1.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     activation\u001b[39m.\u001b[39mappend(output)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X22sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m model\u001b[39m.\u001b[39mfc\u001b[39m.\u001b[39mregister_forward_hook(caching_hook)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X22sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m out \u001b[39m=\u001b[39m model(input_tensor)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X22sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# model.run_with_hooks(\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X22sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m#     fwd_hooks=[(f\"blocks.{layer}.mlp.hook_post\", caching_hook)]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X22sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X22sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mreturn\u001b[39;00m activation\n",
      "File \u001b[1;32mc:\\Users\\neeraj.saini\\AppData\\Local\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\neeraj.saini\\AppData\\Local\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\neeraj.saini\\Desktop\\New folder\\DeepD\\demo1.ipynb Cell 17\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X22sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m32\u001b[39m \u001b[39m*\u001b[39m \u001b[39m32\u001b[39m \u001b[39m*\u001b[39m \u001b[39m24\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X22sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# Feed to fully-connected layer to predict class\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X22sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X22sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Return class probabilities via a log_softmax function \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neeraj.saini/Desktop/New%20folder/DeepD/demo1.ipynb#X22sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mlog_softmax(x, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\neeraj.saini\\AppData\\Local\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\neeraj.saini\\AppData\\Local\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1581\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1579\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, args, kwargs, result)\n\u001b[0;32m   1580\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1581\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, args, result)\n\u001b[0;32m   1583\u001b[0m \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1584\u001b[0m     result \u001b[39m=\u001b[39m hook_result\n",
      "\u001b[1;31mTypeError\u001b[0m: caching_hook() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "get_neuron_acts_layer(input_tensor, 'conv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_hooks(\n",
    "        self,\n",
    "        *model_args,\n",
    "        fwd_hooks: List[Tuple[Union[str, Callable], Callable]] = [],\n",
    "        bwd_hooks: List[Tuple[Union[str, Callable], Callable]] = [],\n",
    "        reset_hooks_end=True,\n",
    "        clear_contexts=False,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "\n",
    "        with self.hooks(\n",
    "            fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts\n",
    "        ) as hooked_model:\n",
    "            return hooked_model.forward(*model_args, **model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations for layer conv1:\n",
      "tensor([[[-1.3593e-01, -3.4356e-01, -2.8973e-01,  ..., -3.8275e-01,\n",
      "          -4.5603e-01, -2.8351e-01],\n",
      "         [-4.2713e-01, -6.4379e-01, -7.7101e-01,  ..., -8.3010e-01,\n",
      "          -7.0007e-01, -5.3686e-01],\n",
      "         [-4.3008e-01, -7.8576e-01, -7.6857e-01,  ..., -8.4137e-01,\n",
      "          -7.6311e-01, -5.9168e-01],\n",
      "         ...,\n",
      "         [-4.5650e-01, -7.8799e-01, -7.1777e-01,  ..., -6.8976e-01,\n",
      "          -7.0851e-01, -4.3310e-01],\n",
      "         [-4.0287e-01, -7.4784e-01, -7.3499e-01,  ..., -7.8667e-01,\n",
      "          -7.3654e-01, -4.9855e-01],\n",
      "         [-3.3910e-01, -5.1722e-01, -5.4743e-01,  ..., -6.4671e-01,\n",
      "          -6.3124e-01, -4.7718e-01]],\n",
      "\n",
      "        [[-2.3386e-01, -4.1002e-01, -4.4086e-01,  ..., -4.1223e-01,\n",
      "          -5.0240e-01, -3.9036e-01],\n",
      "         [-3.9593e-01, -6.4662e-01, -6.3938e-01,  ..., -8.2506e-01,\n",
      "          -8.2114e-01, -5.8833e-01],\n",
      "         [-3.9094e-01, -6.3415e-01, -8.1795e-01,  ..., -7.2057e-01,\n",
      "          -7.5762e-01, -5.4745e-01],\n",
      "         ...,\n",
      "         [-3.9676e-01, -6.1500e-01, -7.3159e-01,  ..., -6.3604e-01,\n",
      "          -7.1067e-01, -4.6639e-01],\n",
      "         [-3.9240e-01, -7.0134e-01, -7.6035e-01,  ..., -6.4958e-01,\n",
      "          -6.6494e-01, -5.8039e-01],\n",
      "         [-3.7134e-01, -5.3968e-01, -5.2711e-01,  ..., -5.6346e-01,\n",
      "          -5.9102e-01, -4.7385e-01]],\n",
      "\n",
      "        [[-4.9568e-01, -6.9682e-01, -7.4095e-01,  ..., -8.1554e-01,\n",
      "          -8.8645e-01, -5.7654e-01],\n",
      "         [-7.8732e-01, -1.0831e+00, -1.1180e+00,  ..., -1.4681e+00,\n",
      "          -1.3095e+00, -6.9748e-01],\n",
      "         [-8.5524e-01, -1.2019e+00, -1.1431e+00,  ..., -1.3527e+00,\n",
      "          -1.1465e+00, -5.7319e-01],\n",
      "         ...,\n",
      "         [-6.8611e-01, -1.1708e+00, -1.1136e+00,  ..., -1.0033e+00,\n",
      "          -1.2554e+00, -7.1048e-01],\n",
      "         [-7.5724e-01, -1.1989e+00, -1.1843e+00,  ..., -9.6660e-01,\n",
      "          -1.1542e+00, -6.5139e-01],\n",
      "         [-6.5788e-01, -7.1453e-01, -7.6793e-01,  ..., -7.3750e-01,\n",
      "          -6.2026e-01, -3.3846e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.2719e-02, -2.7618e-01, -2.2609e-01,  ..., -1.6703e-01,\n",
      "          -2.4646e-01, -1.3041e-01],\n",
      "         [-4.0519e-01, -6.4841e-01, -7.9803e-01,  ..., -9.1130e-01,\n",
      "          -7.8035e-01, -3.9261e-01],\n",
      "         [-4.2300e-01, -8.3590e-01, -9.1190e-01,  ..., -9.4443e-01,\n",
      "          -9.0158e-01, -4.5716e-01],\n",
      "         ...,\n",
      "         [-3.9032e-01, -6.8330e-01, -8.4765e-01,  ..., -6.3987e-01,\n",
      "          -6.8965e-01, -4.4444e-01],\n",
      "         [-4.2913e-01, -7.6994e-01, -8.7843e-01,  ..., -6.8172e-01,\n",
      "          -8.8565e-01, -4.5855e-01],\n",
      "         [-5.1414e-01, -5.8790e-01, -5.3984e-01,  ..., -5.6373e-01,\n",
      "          -5.9120e-01, -2.8116e-01]],\n",
      "\n",
      "        [[ 1.9788e-01,  8.7161e-02,  2.2878e-01,  ...,  2.5255e-01,\n",
      "           1.3176e-01, -1.1064e-01],\n",
      "         [ 2.3068e-01,  2.2319e-01,  6.8260e-02,  ...,  4.0166e-01,\n",
      "           5.4258e-01, -9.4050e-02],\n",
      "         [ 2.5881e-01,  4.7935e-02, -7.2074e-02,  ..., -7.3040e-02,\n",
      "           2.5049e-01, -8.1970e-02],\n",
      "         ...,\n",
      "         [ 4.3927e-02,  5.7103e-02,  1.3499e-01,  ...,  2.7695e-01,\n",
      "           4.7668e-01, -3.8155e-05],\n",
      "         [ 3.1507e-01,  4.2702e-02,  6.9893e-02,  ...,  2.9734e-01,\n",
      "           2.5949e-01, -8.1995e-04],\n",
      "         [ 2.7990e-01,  1.0049e-01, -8.6065e-02,  ...,  5.1711e-02,\n",
      "           1.6075e-01, -7.5535e-03]],\n",
      "\n",
      "        [[-3.8531e-01, -7.4474e-01, -7.0025e-01,  ..., -9.0715e-01,\n",
      "          -8.4301e-01, -5.8512e-01],\n",
      "         [-6.0197e-01, -9.1966e-01, -9.9932e-01,  ..., -1.0812e+00,\n",
      "          -1.0026e+00, -6.7746e-01],\n",
      "         [-5.8510e-01, -9.3196e-01, -9.5925e-01,  ..., -1.0641e+00,\n",
      "          -8.9370e-01, -6.9375e-01],\n",
      "         ...,\n",
      "         [-6.1437e-01, -9.0464e-01, -9.4954e-01,  ..., -9.6063e-01,\n",
      "          -1.0090e+00, -6.9137e-01],\n",
      "         [-6.9954e-01, -9.7129e-01, -9.4908e-01,  ..., -9.9864e-01,\n",
      "          -1.0072e+00, -6.8694e-01],\n",
      "         [-5.5639e-01, -6.8430e-01, -6.0798e-01,  ..., -6.6916e-01,\n",
      "          -7.0011e-01, -4.7773e-01]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neeraj.saini\\AppData\\Local\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\functional.py:1352: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
      "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# # Define a simple neural network (just an example)\n",
    "# class SimpleNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SimpleNet, self).__init__()\n",
    "#         self.fc1 = nn.Linear(10, 20)\n",
    "#         self.fc2 = nn.Linear(20, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# # Create an instance of the model\n",
    "# model = SimpleNet()\n",
    "\n",
    "# # Sample input data\n",
    "# input_data = torch.randn(1, 10)  # Assuming input size is (1, 10)\n",
    "\n",
    "# Dictionary to store activations for each layer\n",
    "activations = {}\n",
    "\n",
    "# Define a function to store the activations\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hooks to store activations\n",
    "target_layer = 1  # Choose the layer you want to examine\n",
    "layer_name = f'conv{target_layer}'\n",
    "model.conv2.register_forward_hook(get_activation(layer_name))\n",
    "\n",
    "# Perform forward pass\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    _ = model(input_tensor)\n",
    "\n",
    "# Access the stored activations for the chosen layer\n",
    "if layer_name in activations:\n",
    "    activation_value = activations[layer_name]\n",
    "    print(f'Activations for layer {layer_name}:\\n{activation_value}')\n",
    "else:\n",
    "    print(f'Layer {layer_name} activations not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 64, 64])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
